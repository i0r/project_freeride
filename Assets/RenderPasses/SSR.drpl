lib SSR {
    properties {
        uint2 OutputSize;
        uint MipCount;
    }
    
    resources {
        sampler LinearSampler {
            format = LinearClampEdge;
        }
        
        RWTexture2D RayHitTarget {
            swizzle = float4;
        }

        Texture2D HiZDepthBuffer {
            swizzle = float;
        }
        
        Texture2D ThinGBuffer {
            swizzle = float4;
        }
        
        // Resolve
        Texture2D RayHitBuffer {
            swizzle = float4;
        }
        
        Texture2D ColorBuffer {
            swizzle = float4;
        }
        
        Texture2D BrdfDfgLut {
            swizzle = float2;
        }
        
        RWTexture2D ResolvedTraceTarget {
            swizzle = float4;
        }
    }
   
    shared {
        #include <GeometryUtils.hlsli>
        #include <Photometry.hlsli>
        
        bool IsSamplingOutsideViewport(float3 raySample, inout float attenuationFactor) {
            // Any rays that march outside the screen viewport will not have any valid pixel information. These need to be dropped.
            float2 UVSamplingAttenuation = smoothstep(0.0, 0.05, raySample.xy) * (1 - smoothstep(0.95, 1, raySample.xy));
            attenuationFactor = UVSamplingAttenuation.x * UVSamplingAttenuation.y;
            return attenuationFactor <= 0;
        }

        float4 RayMarch(float3 worldReflectionVec, float3 screenSpaceReflectionVec, float3 screenSpacePos)
        {
            const float kMaxRayMarchStep = 0.04;
            const int kMaxRayMarchIterations = 25;
            const int kMaxBinarySearchSamples = 8;

            bool bFoundIntersection = false;
            float3 minraySample = float3(0.0, 0.0, 0.0);
            float3 maxraySample = float3(0.0, 0.0, 0.0);

            float viewportAttenuationFactor = 1.0f;

            // Raymarch in the direction of the ScreenSpaceReflectionVec until you get an intersection with your z buffer
            for (int rayStepIdx = 0; rayStepIdx < kMaxRayMarchIterations; rayStepIdx++) {
                float3 offset = float(rayStepIdx) * kMaxRayMarchStep * screenSpaceReflectionVec;
                float3 raySample = screenSpacePos + offset;

                if (IsSamplingOutsideViewport(raySample, viewportAttenuationFactor)) {
                    return float4( 0, 0, 0, viewportAttenuationFactor );
                }

                float ZBufferVal = HiZDepthBuffer.SampleLevel( LinearSampler, raySample.xy, 0.0f ).r;

                // We have stepped onto a territory without any geometry, hence the default depth value
                if (ZBufferVal == 0.0 || ZBufferVal == 1.0) {
                    return float4( 0, 0, 0, 0 );
                }

                maxraySample = raySample;

                float bias = rayStepIdx == 0 ? 0.0001 : 0.0;
                if (raySample.z > ZBufferVal + bias) {
                    bFoundIntersection = true;
                    break;
                }

                minraySample = maxraySample;
            }

            // Binary search
            if (bFoundIntersection) {
                float3 midraySample;
                for (int i = 0; i < kMaxBinarySearchSamples; i++) {
                    midraySample = lerp(minraySample, maxraySample, 0.5);
                    float ZBufferVal = HiZDepthBuffer.SampleLevel( LinearSampler, midraySample.xy, 0.0f ).r;

                    if (midraySample.z > ZBufferVal) {
                        maxraySample = midraySample;
                    } else {
                        minraySample = midraySample;
                    }
                }

                return float4( midraySample, viewportAttenuationFactor );
            }

            return float4( 0, 0, 0, 0 );
        }

        float4 ComputeHitTrace( const float3 N, const float Depth, const float3 WorldPosition, const float2 UvCoords )
        {
            float3 V = GetViewDir( WorldPosition );
            float4 positionScreenSpace = float4( UvCoords, Depth, 1.0f );
            float3 R = reflect( V, N );
            
            float attenuationFactor = 1.0f - smoothstep(0.25f, 0.5f, dot(-V, R));
            bool isReflectedBackToCam = (attenuationFactor <= 0.0f);
            if ( isReflectedBackToCam ) {
                return float4( 0, 0, 0, 0 );
            }
            
            float4 pointAlongReflectionVec = float4(10.0f * R + WorldPosition, 1.0f);
    
            float4 screenSpaceReflectionPoint = mul( g_ViewProjectionMatrix, pointAlongReflectionVec );
            screenSpaceReflectionPoint /= screenSpaceReflectionPoint.w;
            screenSpaceReflectionPoint.xyz = screenSpaceReflectionPoint.xyz * 0.5 + 0.5; // To [0; 1]

            // Compute the sreen space reflection vector as the difference of the two screen space points
            float3 screenSpaceReflectionVec = normalize(screenSpaceReflectionPoint.xyz - positionScreenSpace.xyz);

            return RayMarch(R, screenSpaceReflectionVec.xyz, positionScreenSpace.xyz);
        }
        
        // http://roar11.com/2015/07/screen-space-glossy-reflections/
        static const float kMaxSpecularExponent = 16.0;
        static const float kSpecularBias = 1.0;

        float SpecularPowerToConeAngle(float specularPower) {
            // based on phong distribution model
            if(specularPower >= exp2(kMaxSpecularExponent)) {
                return (0.0f);
            }
            const float xi = 0.244f;
            float exponent = 1.0f / (specularPower + 1.0f);
            return acos(pow(xi, exponent));
        }

        // https://seblagarde.wordpress.com/2012/06/10/amd-cubemapgen-for-physically-based-rendering/
        float RoughnessToSpecularPower(float roughness) {
            float gloss = 1.0f - roughness;
            return exp2(kMaxSpecularExponent * gloss + kSpecularBias);
        }

        float IsoscelesTriangleOpposite(float adjacentLength, float coneTheta) {
            return (2.0f * tan(coneTheta) * adjacentLength);
        }

        // a - opposite length of the triangle
        // h - adjacent length of the triangle
        float IsoscelesTriangleInscribedCircleRadius(float a, float h) {
            float a2 = a * a;
            float fh2 = 4.0f * h * h;
            return (a * (sqrt(a2 + fh2) - a)) / (4.0f * h);
        }

        float4 ConeSampleWeightedColor(float2 samplePos, float mipChannel, float gloss) {
            float3 sampleColor = ColorBuffer.SampleLevel( LinearSampler, samplePos, mipChannel ).rgb;
            return float4(sampleColor * gloss, gloss);
        }

        float IsoscelesTriangleNextAdjacent(float adjacentLength, float incircleRadius) {
            // subtract the diameter of the incircle to get the adjacent side of the next level on the cone
            return adjacentLength - (incircleRadius * 2.0f);
        }

        float3 TraceCones(float4 rayHitInfo, float2 UvCoords, float roughness) 
        {
            if (rayHitInfo.a == 0.0) {
                return float3(0.0, 0.0, 0.0);
            }

            float depth = HiZDepthBuffer.SampleLevel( LinearSampler, UvCoords, 0.0f ).r;
            
            float3 raySS = rayHitInfo.xyz;
            float3 positionSS = float3(UvCoords, depth);

            // Get specular power from roughness
            float gloss = 1.0f - roughness;
            float specularPower = RoughnessToSpecularPower(roughness);

            // Convert to cone angle (maximum extent of the specular lobe aperture)
            // Only want half the full cone angle since we're slicing the isosceles triangle in half to get a right triangle
            float coneTheta = SpecularPowerToConeAngle(specularPower) * 0.5f;

            // P1 = positionSS, P2 = raySS, adjacent length = ||P2 - P1||
            float2 deltaP = raySS.xy - positionSS.xy;
            float adjacentLength = length(deltaP);
            float2 adjacentUnit = normalize(deltaP);

            float4 totalReflectedColor = float4(0.0, 0.0, 0.0, 0.0);
            float remainingAlpha = 1.0f;
            float maxMipLevel = float(MipCount) - 1.0f;
            float glossMult = gloss;

            // Cone-tracing using an isosceles triangle to approximate a cone in screen space
            for(int i = 0; i < 7; ++i) {
                // Intersection length is the adjacent side, get the opposite side using trig
                float oppositeLength = IsoscelesTriangleOpposite(adjacentLength, coneTheta);

                // Calculate in-radius of the isosceles triangle
                float incircleSize = IsoscelesTriangleInscribedCircleRadius(oppositeLength, adjacentLength);

                // Get the sample position in screen space
                float2 samplePos = positionSS.xy + adjacentUnit * (adjacentLength - incircleSize);

                // Convert the in-radius into screen size then check what power N to raise 2 to reach it - that power N becomes mip level to sample from
                float mipChannel = clamp(log2(incircleSize * max(OutputSize.x, OutputSize.y)), 0.0, maxMipLevel);

                /*
                 * Read color and accumulate it using trilinear filtering and weight it.
                 * Uses pre-convolved image (color buffer) and glossiness to weigh color contributions.
                 * Visibility is accumulated in the alpha channel. Break if visibility is 100% or greater (>= 1.0f).
                 */
                float4 newColor = ConeSampleWeightedColor(samplePos, mipChannel, glossMult);

                remainingAlpha -= newColor.a;

                if(remainingAlpha < 0.0f) {
                    newColor.rgb *= (1.0f - abs(remainingAlpha));
                }

                totalReflectedColor += newColor;

                if(totalReflectedColor.a >= 1.0f) {
                    break;
                }

                adjacentLength = IsoscelesTriangleNextAdjacent(adjacentLength, incircleSize);
                glossMult *= gloss;
            }

            return totalReflectedColor.rgb * rayHitInfo.a;
        }
    }
    
    shader HiZTraceCS {
        float2 pixelCoordinates = float2( $SV_DispatchThreadId.xy );
        float2 uvCoordinates = pixelCoordinates / OutputSize;
        float2 ndcCoordinates = float2( 2.0f, -2.0f ) * uvCoordinates + float2( -1.0f, 1.0f );
        
        float4 ThinGBufferSample = ThinGBuffer.SampleLevel( LinearSampler, uvCoordinates, 0.0f );  
        float3 N = DecodeNormals( ThinGBufferSample.rg );
        float  roughness = ThinGBufferSample.b;
        float  depth = HiZDepthBuffer.SampleLevel( LinearSampler, uvCoordinates, 0.0f ).r;
        
        float3 screenPos = GetScreenPos( uvCoordinates, depth );
        float3 worldPos = ReconstructWorldPos( uvCoordinates, depth );
        
        RayHitTarget[pixelCoordinates] = ComputeHitTrace( N, depth, worldPos, uvCoordinates );
    }

    shader ResolveTraceCS {
        float2 pixelCoordinates = float2( $SV_DispatchThreadId.xy ) + float2( 0.5f, 0.5f );
        float2 uvCoordinates = pixelCoordinates / OutputSize;
        
        float4 rayHit = RayHitBuffer[pixelCoordinates];
        
        float4 ThinGBufferSample = ThinGBuffer.SampleLevel( LinearSampler, uvCoordinates, 0.0f );  
        float3 N = DecodeNormals( ThinGBufferSample.rg );
        float  roughness = ThinGBufferSample.b;
        float  depth = HiZDepthBuffer.SampleLevel( LinearSampler, uvCoordinates, 0.0f ).r;
        
        float3 screenPos = GetScreenPos( uvCoordinates, depth );
        float3 worldPos = ReconstructWorldPos( uvCoordinates, depth );
        float3 reflectedPointWorldPosition = ReconstructWorldPos( rayHit.xy, depth );
        float3 V = GetViewDir( worldPos );
        
        float3 H;

        float attenuation = rayHit.a;
        bool rayHitDetected = attenuation > 0.0;

        if (rayHitDetected) {
            // Compute a proper half-vector if it's possible using reflected point's position.
            float3 L = normalize(reflectedPointWorldPosition - worldPos);
            H = normalize(L + V);
        } else {
            // If not, just take a normal instead.
            H = normalize(N + V);
        }
        
        float NoV = saturate(dot(N, V));

        float3 sourceColor = ColorBuffer[pixelCoordinates].rgb;
        
        float3 reflectedColor = TraceCones(rayHit, uvCoordinates, roughness);
        
        // Rebuild DFG function.
        float F0 = ThinGBufferSample.aaa;
        float smoothness = 1.0f - roughness;
        
        float2 f_ab = BrdfDfgLut.SampleLevel( LinearSampler, float2( NoV, roughness ), 0.0f );
        float3 Fr = max(float3( smoothness, smoothness, smoothness ), F0) - F0;
        float3 k_S = F0 + Fr * Pow5(1.0 - NoV);
        
        float3 FssEss = k_S * f_ab.x + f_ab.y;
        
        float Ems = (1.0 - (f_ab.x + f_ab.y));
        float3 F_avg = F0 + (1.0 - F0) / 21.0;
        float3 FmsEms = Ems * FssEss * F_avg / (1.0 - F_avg * Ems);
        
        float mask = lerp(attenuation * attenuation, 0.0f, smoothstep(0.6f, 1.2f, roughness));
        float3 reflectionColor = ( FmsEms + (reflectedColor * (1.0 - FssEss - FmsEms)) ) * mask;
        
        ResolvedTraceTarget[pixelCoordinates] = float4( sourceColor + reflectionColor, 1.0f );
    }
    
    pass HiZTrace {
        compute = HiZTraceCS;
        dispatch = { 16, 16, 1 };
    }
    
    pass ResolveTrace {
        compute = ResolveTraceCS;
        dispatch = { 16, 16, 1 };
    }
}
