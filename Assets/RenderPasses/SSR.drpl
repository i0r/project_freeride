lib SSR {
    properties {
        uint2 OutputSize;
        float2 HaltonOffset;
    }
    
    resources {
        sampler LinearSampler {
            format = LinearClampEdge;
        }
        
        RWTexture2D RayHitTarget {
            swizzle = float4;
        }
        
        RWTexture2D MaskTarget {
            swizzle = float;
        }
        
        Texture2D HiZDepthBuffer {
            swizzle = float;
        }
        
        Texture2D ThinGBuffer {
            swizzle = float4;
        }
        
        Texture2D BlueNoise {
            swizzle = float2;
        }
        
        // Resolve Pass
        Texture2D ColorBuffer {
            swizzle = float4;
        }
        
        Texture2D RayTraceBuffer {
            swizzle = float4;
        }
        
        Texture2D MaskBuffer {
            swizzle = float;
        }
        
        RWTexture2D ResolvedOutput {
            swizzle = float4;
        }
    }
   
    shared {
        #include <GeometryUtils.hlsli>
        
        static const float Weight[9] = {
            1.0f / 16,
            1.0f / 8,
            1.0f / 16,
            1.0f / 8,
            1.0f / 4,
            1.0f / 8,
            1.0f / 16,
            1.0f / 8,
            1.0f / 16
        };
        
        static const float2 Offset[4] =	
        {	
            float2(0, 0),	
            float2(2, -2),	
            float2(-2, -2),	
            float2(0, 2)	
        };
        
        static const int NUM_STEPS = 300;
        static const float BRDF_BIAS = 0.7f;
        static const float RAY_BIAS = 0.05f;
        static const int HIZ_START_LEVEL = 0;
        static const int HIZ_STOP_LEVEL = 0;
        static const float THICKNESS = 50.0f;
        static const int BLUE_NOISE_SIZE = 1024;
        static const float TEMPORAL_WEIGHT = 0.99f;
        static const float TEMPORAL_CLAMP_SCALE = 3.00f;
        static const float EXPOSURE = 1.0f;
        static const int NUM_RESOLVE = 4;
        
        inline float SmithJointGGXVisibilityTerm(float NdotL, float NdotV, float roughness)
        {
            float a = roughness;
            float lambdaV = NdotL * (NdotV * (1 - a) + a);
            float lambdaL = NdotV * (NdotL * (1 - a) + a);

            return (0.5f / (lambdaV + lambdaL + 1e-5f));
        }

        inline float GGXTerm(float NdotH, float roughness)
        {
            float a2 = roughness * roughness;
            float d = (NdotH * a2 - NdotH) * NdotH + 1.0f; // 2 mad
            return a2 / (PI * (d * d + 1e-7f)); // This function is not intended to be running on Mobile,
            // therefore epsilon is smaller than what can be represented by float
        }
 
        float BRDF_Weight(float3 V, float3 L, float3 N, float Roughness)
        {
            float3 H = normalize(L + V);

            float NdotH = saturate(dot(N, H));
            float NdotL = saturate(dot(N, L));
            float NdotV = saturate(dot(N, V));

            float G = SmithJointGGXVisibilityTerm(NdotL, NdotV, Roughness);
            float D = GGXTerm(NdotH, Roughness);

            return (D * G) * (PI / 4.0);
        }
        
        float4 ImportanceSampleGGX(float2 Xi, float Roughness) {
            float m = Roughness * Roughness;
            float m2 = m * m;

            float Phi = 2 * PI * Xi.x;

            float CosTheta = sqrt((1.0 - Xi.y) / (1.0 + (m2 - 1.0) * Xi.y));
            float SinTheta = sqrt(max(1e-5, 1.0 - CosTheta * CosTheta));

            float3 H;
            H.x = SinTheta * cos(Phi);
            H.y = SinTheta * sin(Phi);
            H.z = CosTheta;

            float d = (CosTheta * m2 - CosTheta) * CosTheta + 1;
            float D = m2 / (PI * d * d);
            float pdf = D * CosTheta;

            return float4(H, pdf);
        }
        
        float2 CalculateMotion(float2 inUV, float rawDepth)
        {
            float4 worldPos = float4(inUV, rawDepth, 1.0f);

            float4 prevClipPos = mul(worldPos, g_PreviousViewProjectionMatrix);
            float4 curClipPos = mul(worldPos, g_ViewProjectionMatrix);

            float2 prevHPos = prevClipPos.xy / prevClipPos.w;
            float2 curHPos = curClipPos.xy / curClipPos.w;

            // V is the viewport position at this pixel in the range 0 to 1.
            float2 vPosPrev = prevHPos.xy / float2(2.0f, -2.0f) + float2(0.5f, 0.5f);
            float2 vPosCur = curHPos.xy / float2(2.0f, -2.0f) + float2(0.5f, 0.5f);
            return vPosCur - vPosPrev;
        }
        
        void ResolveAABB
        (
            Texture2D currColor,
            float AABBScale,
            float2 uv,
            float2 ScreenSize,
            inout float Variance,
            inout float4 MinColor,
            inout float4 MaxColor,
            inout float4 FilterColor
        )
        {
            const int2 SampleOffset[9] = {
                int2(-1.0, -1.0),
                int2(0.0, -1.0),
                int2(1.0, -1.0),
                int2(-1.0, 0.0),
                int2(0.0, 0.0),
                int2(1.0, 0.0),
                int2(-1.0, 1.0),
                int2(0.0, 1.0),
                int2(1.0, 1.0)
            };

            float4 SampleColors[9];

            for (uint i = 0; i < 9; i++)
            {
                SampleColors[i] = currColor.Load(int3( uv + (SampleOffset[i] / ScreenSize), 0 ) );
            }

            half4 m1 = 0.0;
            half4 m2 = 0.0;
            for (uint x = 0; x < 9; x++)
            {
                m1 += SampleColors[x];
                m2 += SampleColors[x] * SampleColors[x];
            }

            half4 mean = m1 / 9.0;
            half4 stddev = sqrt((m2 / 9.0) - mean * mean);

            MinColor = mean - AABBScale * stddev;
            MaxColor = mean + AABBScale * stddev;

            FilterColor = SampleColors[4];
            MinColor = min(MinColor, FilterColor);
            MaxColor = max(MaxColor, FilterColor);
        }
        
        //
        //
        //
        //
        static const float MAX_REFLECTION_RAY_MARCH_STEP = 0.02f;
        static const float RAY_MARH_BIAS = 0.001f;
        static const float SPEC_INTENSITY_TO_REFLECTANCE = 0.5f;
        static const int  NumDepthMips = 7;
        
        void StepThroughCell(inout float3 RaySample, float3 RayDir, int MipLevel)
        {
            // Size of current mip 
            int2 MipSize = int2(OutputSize) >> MipLevel;

            // UV converted to index in the mip
            float2 MipCellIndex = RaySample.xy * float2(MipSize);

            // The cell boundary UV in the direction of the ray
            float2 BoundaryUV;
            BoundaryUV.x = RayDir.x > 0 ? ceil(MipCellIndex.x) / float(MipSize.x) : floor(MipCellIndex.x) / float(MipSize.x);
            BoundaryUV.y = RayDir.y > 0 ? ceil(MipCellIndex.y) / float(MipSize.y) : floor(MipCellIndex.y) / float(MipSize.y);

            // Result of 2 ray-line intersections where we intersect the ray with the boundary cell UVs
            float2 ParameterUV;
            ParameterUV.x = (BoundaryUV.x - RaySample.x) / RayDir.x;
            ParameterUV.y = (BoundaryUV.y - RaySample.y) / RayDir.y;

            float CellStepOffset = 0.05;

            // Pick the cell intersection that is closer, and march to that cell
            if (abs(ParameterUV.x) < abs(ParameterUV.y))
            {
                RaySample += (ParameterUV.x + CellStepOffset) * RayDir;
            }
            else
            {
                RaySample += (ParameterUV.y + CellStepOffset) * RayDir;
            }
        }

        void Raymarch(
            float2 InUV,
            float4 ScreenSpacePos,
            float ReflectionRayMarchStep,
            float3 WorldPosition,
            float3 WorldNormal,
            float3 CameraVector,
            float4x4 ViewProjectionMat,
            out float2 ReflectionUV,
            out float Attenuation,
            out float FilterSize)
        {
            ReflectionUV = 0;
            Attenuation = 0;
            FilterSize = 0;

            // Compute world space reflection vector
            float3 ReflectionVector = reflect(CameraVector, WorldNormal.xyz);

            // This will check the direction of the reflection vector with the view direction,
            // and if they are pointing in the same direction, it will drown out those reflections 
            // since we are limited to pixels visible on screen. Attenuate reflections for angles between 
            // 60 degrees and 75 degrees, and drop all contribution beyond the (-60,60)  degree range
            float CameraFacingReflectionAttenuation = 1 - smoothstep(0.25, 0.5, dot(-CameraVector, ReflectionVector));

            // Reject if the reflection vector is pointing back at the viewer.
            [branch]
            if (CameraFacingReflectionAttenuation <= 0)
                return;

            // Compute second sreen space point so that we can get the SS reflection vector
            float4 ScreenSpaceReflectionPoint = mul(ViewProjectionMat, float4(10.f*ReflectionVector + WorldPosition, 1.f));
            ScreenSpaceReflectionPoint /= ScreenSpaceReflectionPoint.w;
            ScreenSpaceReflectionPoint.xy = ScreenSpaceReflectionPoint.xy * float2(0.5, -0.5) + float2(0.5, 0.5);

            // Compute the sreen space reflection vector as the difference of the two screen space points
            float3 ScreenSpaceReflectionVec = normalize(ScreenSpaceReflectionPoint.xyz - ScreenSpacePos.xyz);

            // Dithered offset for raymarching to prevent banding artifacts
            float DitherOffset = BlueNoise.SampleLevel(LinearSampler, InUV * HaltonOffset, 0).r * 0.01f + RAY_MARH_BIAS;
      
            float3 RaySample = ScreenSpacePos.xyz + DitherOffset * ScreenSpaceReflectionVec;
            float2 UVSamplingAttenuation = float2(0.0, 0.0);

            int MipLevel = 0;
            int IterCount = 0;
            while (	MipLevel > -1 && 
                    MipLevel < (NumDepthMips - 1) && 
                    IterCount < 20)
            {
                StepThroughCell(RaySample, ScreenSpaceReflectionVec, MipLevel);

                UVSamplingAttenuation = smoothstep(0.05, 0.1, RaySample.xy) * (1 - smoothstep(0.95, 1, RaySample.xy));
                UVSamplingAttenuation.x *= UVSamplingAttenuation.y;

                if (UVSamplingAttenuation.x > 0)
                {
                    float ZBufferValue = HiZDepthBuffer.SampleLevel(LinearSampler, RaySample.xy, MipLevel).r;

                    if (RaySample.z < ZBufferValue)
                    {
                        MipLevel++;
                    }
                    else
                    {
                        float t = (RaySample.z - ZBufferValue) / ScreenSpaceReflectionVec.z;
                        RaySample -= ScreenSpaceReflectionVec * t;
                        MipLevel--;
                    }

                    IterCount++;
                }
                else
                {
                    break;
                }
            }


            [branch]
            if (MipLevel == -1)
            {
                // Screenspace UV of the reflected color
                ReflectionUV = RaySample.xy;

                // float BitMaskValue = Gbuf_DiffuseTex.SampleLevel(PointSamp, ReflectionUV, 0).a;

                // // Bail out if reflected pixel is from a mover primitive, and the normal is not upward facing.
                // // This is to restrict movers to floor reflections only since wall reflections create a lot of artifacting. 
                // [branch]
                // if (WorldNormal.z < 0.996f)
                // {
                    // return;
                // }

                // Use gloss value to figure out how blurry the reflections should be
                // Gloss values of 50 or more result in mirror reflections. Mapping or 
                // Filter = -0.6*Gloss + 31 was obtained by solving linear equation that 
                // maps Gloss=10 to FilterSize=25, and Gloss=50 to FilterSize=1
                //float4 SpecularGBufferVal = Gbuf_SpecularTex.SampleLevel(PointSamp, InUV, 0);
                float Gloss = 1.0f; //DecodeSpecularPower(SpecularGBufferVal);
                FilterSize = max(1, -0.6*Gloss + 31);

                // Use specular intensity to figure out the reflectance 
                float Reflectance = 1.0f; //SPEC_INTENSITY_TO_REFLECTANCE * SpecularGBufferVal.r;

                // This will check the direction of the normal of the reflection sample with the
                // direction of the reflection vector, and if they are pointing in the same direction,
                // it will drown out those reflections since backward facing pixels are not available 
                // for screen space reflection. Attenuate reflections for angles between 90 degrees 
                // and 100 degrees, and drop all contribution beyond the (-100,100)  degree range
                
                float4 thinGbuffer = ThinGBuffer.SampleLevel(LinearSampler, ReflectionUV, 0);
                float3 ReflectionNormal = DecodeNormals( thinGbuffer.rg );
                
                //float4 ReflectionNormalColor = Gbuf_WorldNormalTex.SampleLevel(PointSamp, ReflectionUV, 0);
                //float4 ReflectionNormal = ReflectionNormalColor * float4(2, 2, 2, 1) - float4(1, 1, 1, 0);
                float DirectionBasedAttenuation = smoothstep(-0.17, 0.0, dot(ReflectionNormal.xyz, -ReflectionVector));

                // Attenuate any reflection color from the foreground. The GBuffer normal color for foreground objects is (0,0,1)
                float ForegroundAttenuation = 1.0f; //step(0.0001f, ReflectionNormalColor.r * ReflectionNormalColor.g);

                // Range based attenuation parameter to gracefully fade out reflections at the edge of ray march range
                float RangeBasedAttenuation = 1.0 - smoothstep(0.5, 1.0, 3.f*length(ReflectionUV - ScreenSpacePos.xy));

                // Use material parameters and normal direction to figure out reflection contribution
                Attenuation = Reflectance * DirectionBasedAttenuation * RangeBasedAttenuation * CameraFacingReflectionAttenuation * UVSamplingAttenuation.x * ForegroundAttenuation;
            }
        }
    }
    
    shader HiZTraceCS {
        uint2 PixelID = uint2($SV_DispatchThreadId.x, $SV_DispatchThreadId.y);
        float2 PixelUV = float2( float2(PixelID) + 0.5f ) / OutputSize;
        float2 NDCPos = float2(2.f,-2.f) * PixelUV + float2(-1.f,1.f);

        // Initialize to 0 as some of the code paths might not write to O/P
        RayHitTarget[PixelID] = float4(0, 0, 0, 0);
        MaskTarget[PixelID] = 0.0f;

        float3 ScreenVector = mul(g_InverseViewProjectionMatrix, float4(NDCPos, 1, 0)).xyz;

        // Compute world position
        float DeviceZ = HiZDepthBuffer[PixelID].r;
        float4 thinGbuffer = ThinGBuffer[PixelID].rgba;
       
        float LinearZ = ConvertFromDeviceZ(DeviceZ, g_ClippingPlanes.x, g_ClippingPlanes.y);
        float3 WorldPosition = ScreenVector * LinearZ;
        
        float3 CameraVector = normalize(ScreenVector);
        float3 WorldNormal = DecodeNormals( thinGbuffer.rg );
        
        // ScreenSpacePos --> (texcoord.xy, device_z)
        float4 ScreenSpacePos = float4(PixelUV, DeviceZ, 1.f);

        float2 OutReflectionUV;
        float OutAttenuation, OutFilterSize;

        Raymarch(PixelUV, ScreenSpacePos, MAX_REFLECTION_RAY_MARCH_STEP, WorldPosition, WorldNormal, CameraVector, g_ViewProjectionMatrix, OutReflectionUV, OutAttenuation, OutFilterSize);

        RayHitTarget[PixelID] = float4(PixelUV, 0, 1); //float4(OutReflectionUV, OutAttenuation, OutFilterSize); //ColorBuffer.SampleLevel(LinearSampler, OutReflectionUV, 0.0f); //float4( CameraVector, 1 ); //
        //RayHitTarget[PixelID] = float4( WorldPosition, 1 );
        
        // float2 pixelCoordinates = float2( $SV_DispatchThreadId.xy ) + float2( 0.5f, 0.5f );
        // float2 uvCoordinates = pixelCoordinates / OutputSize;
        
        // float  depth = HiZDepthBuffer.SampleLevel( LinearSampler, uvCoordinates, 0.0f );
        // float4 thinGbuffer = ThinGBuffer.SampleLevel( LinearSampler, uvCoordinates, 0.0f );
        // float2 hash = BlueNoise.SampleLevel(LinearSampler, (uvCoordinates + HaltonOffset.xy) * OutputSize.xy / float2(BLUE_NOISE_SIZE, BLUE_NOISE_SIZE), 0.0f).rg;
        
        // float2 ParamRenderTargetSize = OutputSize;
        // float3 ParamViewDirection = g_ViewDirection;
        // float3 ParamRightVector = g_RightVector;
        // float3 ParamUpVector = g_UpVector;
        // float3 ParamWorldPosition = g_WorldPosition;
        // float ParamAspectRatio = g_AspectRatio;
        
        // // Generate a view ray (in world space).
        // float xPixelCoordShifted = (2 * (($SV_DispatchThreadId.x + 0.5f) / ParamRenderTargetSize.x) - 1) * ParamAspectRatio; 
        // float yPixelCoordShifted = (2 * (($SV_DispatchThreadId.y + 0.5f) / ParamRenderTargetSize.y) - 1); 

        // float3 rightShift = mul(xPixelCoordShifted, -ParamRightVector);
        // float3 upShift = mul(yPixelCoordShifted, -ParamUpVector);
        // float3 viewDirection =  normalize(ParamViewDirection + rightShift + upShift).xyz;
        
        // //float depthWs = -(g_ClippingPlanes.x * g_ClippingPlanes.y) / (depth * (g_ClippingPlanes.x - g_ClippingPlanes.y) - g_ClippingPlanes.x);	
        // //float distance_to_camera = depthWs / dot(viewDirection, ParamViewDirection);
        // //float3 positionWs = ParamWorldPosition + viewDirection * distance_to_camera;
     
        // float4 position = mul( float4( screenPos, 1 ), g_InverseViewProjectionMatrix );
        // float3 positionWs = viewDirection; //( position / position.w ).xyz;
        
        // float roughness = 0.0f; // thinGbuffer.a;    
        // bool isMirror = ( roughness < 0.1f );
       
        // // Use a stochastic approach for rough surfaces. If roughness is below the threshold; assume the surface
        // // is mirror like and should reflect as is.
        // hash.y = lerp(hash.y, 0.0, BRDF_BIAS);
 
        // float3 normalsVS = DecodeNormals( thinGbuffer.rg );
        // float4 H = ( isMirror ) ? float4( normalsVS, 1.0f ) : TangentToWorld( normalsVS, ImportanceSampleGGX( hash, roughness ) );
        
        // float3 R = reflect(normalize(WorldPosition), H.xyz);

        // float3 screenPos = GetScreenPos(PixelUV, DeviceZ);   
        // float3 rayStart = float3(PixelUV, DeviceZ);
        // float4 rayProj = mul(float4(WorldPosition + R, 1.0), g_ProjectionMatrix);
        // float3 rayDir = normalize((rayProj.xyz / rayProj.w) - screenPos);
        // rayDir.xy *= float2(0.5f, -0.5f);

        // float4 rayTrace = RayTrace(SSR_MAX_MIP_LEVEL, HIZ_START_LEVEL, HIZ_STOP_LEVEL, NUM_STEPS, THICKNESS, OutputSize, rayStart, rayDir);
        // float4 outRayCast = rayTrace;
          
        // RayHitTarget[$SV_DispatchThreadId.xy] = float4(rayTrace.xyz, H.a);
        // MaskTarget[$SV_DispatchThreadId.xy] = rayTrace.a * rayTrace.a;
    }

    shader ResolveCS {
        float2 PixelCoordinates = float2( $SV_DispatchThreadId.xy ) + float2( 0.5f, 0.5f );
        float2 uvCoordinates = PixelCoordinates / OutputSize;
        
        float4 ThinGBufferSample = ThinGBuffer.SampleLevel( LinearSampler, uvCoordinates, 0.0f );
        
        float3 NormalsViewSpace = DecodeNormals( ThinGBufferSample.rg );
        
        float  depth = HiZDepthBuffer.SampleLevel( LinearSampler, uvCoordinates, 0.0f );
        float  roughness = ThinGBufferSample.a;
        
        float2 Hash = BlueNoise.SampleLevel(LinearSampler, (uvCoordinates + HaltonOffset.xy) * OutputSize.xy / float2(BLUE_NOISE_SIZE, BLUE_NOISE_SIZE), 0.0f).rg * 2.0 - 1.0;
        float2x2 offsetRotationMatrix = float2x2(Hash.x, Hash.y, -Hash.y, Hash.x);

        float2 ParamRenderTargetSize = OutputSize;
        float3 ParamViewDirection = g_ViewDirection;
        float3 ParamRightVector = g_RightVector;
        float3 ParamUpVector = g_UpVector;
        float3 ParamWorldPosition = g_WorldPosition;
        float ParamAspectRatio = g_AspectRatio;
        
        // Generate a view ray (in world space).
        float xPixelCoordShifted = (2 * (($SV_DispatchThreadId.x + 0.5f) / ParamRenderTargetSize.x) - 1) * ParamAspectRatio; 
        float yPixelCoordShifted = (2 * (($SV_DispatchThreadId.y + 0.5f) / ParamRenderTargetSize.y) - 1); 

        float3 rightShift = mul(xPixelCoordShifted, -ParamRightVector);
        float3 upShift = mul(yPixelCoordShifted, -ParamUpVector);
        float3 viewDirection =  normalize(ParamViewDirection + rightShift + upShift).xzy;
        float depthWs = -(g_ClippingPlanes.x * g_ClippingPlanes.y) / (depth * (g_ClippingPlanes.y - g_ClippingPlanes.x) - g_ClippingPlanes.y);	
        float distance_to_camera = depthWs / dot(viewDirection, ParamViewDirection);
        float3 viewPos = ParamWorldPosition + viewDirection * distance_to_camera;

        float3 screenPos = GetScreenPos( uvCoordinates, depth );       
        
        float NdotV = saturate(dot(NormalsViewSpace, -viewDirection));
        float coneTangent = lerp(0.0, roughness * (1.0 - BRDF_BIAS), NdotV * sqrt(roughness));

        float maxMipLevel = (float)SSR_MAX_MIP_LEVEL - 1.0;

        float4 result = 0.0;
        float weightSum = 0.0;
        for (int i = 1; i < NUM_RESOLVE; i++)
        {
            float2 offsetUV = Offset[i] * (1.0f / OutputSize);
            offsetUV = mul(offsetUV, offsetRotationMatrix);
            
            float2 neighborUv = uvCoordinates + offsetUV;

            float4 hitPacked = RayTraceBuffer.SampleLevel(LinearSampler, neighborUv, 0.0f);
            float2 hitUv = hitPacked.xy;
            float hitZ = hitPacked.z;
            float hitPDF = hitPacked.w;
            float hitMask = MaskBuffer.SampleLevel(LinearSampler, neighborUv, 0.0f).r;

            float3 hitViewPos = GetViewPos(GetScreenPos(hitUv, hitZ));

            float weight = BRDF_Weight(normalize(-viewPos) /*V*/, normalize(hitViewPos - viewPos) /*L*/, NormalsViewSpace /*N*/, roughness) / max(1e-5, hitPDF);

            float4 sampleColor = float4(0.0, 0.0, 0.0, 1.0);
            sampleColor = ColorBuffer.SampleLevel(LinearSampler, hitUv, 0.0f);
            sampleColor.a = hitMask;
            sampleColor.rgb /= 1 + RGBToLuminance(sampleColor.rgb);
            
            result += sampleColor * weight;
            weightSum += weight;
        }
        result /= weightSum;

        result.rgb /= 1 - RGBToLuminance(result.rgb);

        float4 hitPacked = RayTraceBuffer.SampleLevel(LinearSampler, uvCoordinates, 0.0f);
        ResolvedOutput[$SV_DispatchThreadId.xy] = hitPacked; //(result.a > 0.01f) ? max(result, 1e-5) : float4(0.0f, 0.0f, 0.0f, 0.0f);
    }
    
    pass HiZTrace {
        compute = HiZTraceCS;
        dispatch = { 16, 16, 1 };
    } 
    
    pass ResolveTrace {
        compute = ResolveCS;
        dispatch = { 16, 16, 1 };
    }
}